\chapter{Reinforcement Learning Preliminaries}
This chapter is dedicated to present a concise theory of reinforcement learning. The first section will show how a certain goal can be formalized as a reward maximization -- one of the ideas which serves as a basic foundation of \ac{RL}. Section \ref{sec:mdp} explains the basics of \ac{MDP}, a general framework used in \ac{RL} problem. Subsequently, an intuition of value and policy iteration will be developed in section \ref{sec:value_iter}. The fourth section will present the extension of \ac{RL} for continuous space. Finally, section \ref{sec:actor} will discuss the actor-critic structure which is a natural representation for control system problem.

\section{Goal as Cost Minimization}
The nature of \ac{RL} is inspired by the way living organisms learn to reach their desired goals. Animals for instance, learn by first acting on the environment, observe the changes that occur, and improve their action iteratively. One example is a circus lion that is tasked to perform acrobatic show while its trainer observing the progress. If the lion successfully executes the task, it will be rewarded with foods. Conversely, punishment will be inflicted whenever it fails. The lion initially has no idea of how to perform the task. However through trial and error, it will follow its instinct to increase the frequency of receiving rewards while trying its best to avoid punishments. In a certain duration of training, the circus lion will be finally able to perform the task flawlessly. 

Now we will formalize above illustration for robotics application. A robot can be described by its states $x_k$ with subscript $k$ denoting time instance. Applying an action $u_k$ will bring the robot to state $x_{k+1}$ with immediate reward $r_{k+1}$. Subsequently, at $k+1$ the robot applies $u_{k+1}$ which yields state $x_{k+2}$ and $r_{k+2}$. This action-state-update iteration is run for infinite time instances. The goal is defined as maximization of cumulative reward the robot receives. In control engineering, reward is usually replaced with cost. In that case the goal is defined as minimization problem. Starting from now, we will define goal as minimization of future cost $J$.

From the sequence of cost obtained over time, we can define a formalization of goal, called expected return. Return $R_t$ is a function that maps the sequence of costs into real number. An example of return is the sum of the costs.

J =  

\section{Markov Decision Process} \label{sec:mdp}
\ac{MDP} is defined as a tuple $\left<X, U, f, \rho \right>$ which satisfies Markov property \cite{babuskaRL}. The detailed explanation of Markov property can be found on \cite{sutton1998reinforcement} section 3.5 but the main idea is that to determine the probability of a state at certain time, it is sufficient to only know the state of previous time instance. The elements of the tuple are:
\begin{itemize}
	\item $X$ is the state space
	\item $U$ is the action space
	\item $f :X \times U \rightarrow X$ is the state transition function (system dynamics) 
	\item $\rho:X \times U \rightarrow \mathbb{R}$ is the reward function
\end{itemize}

In control engineering, $f$ represents the system dynamics which is a transition function mapping a current state and action to the one-step ahead state up to a probability distribution. This probability distribution is mathematically denoted in Equation \eqref{eq:markov}.

\begin{equation}
	\text{Pr}\{x_{t+1} = x', r_{t+1} = r| x_t, u_t \}
	\label{eq:markov}
\end{equation}
where $x$ denotes state, $u$ denotes action, and $r$ denotes immediate reward obtained upon applying the input on the corresponding state.

\section{Value and Policy Iteration} \label{sec:value_iter}
Value function describes how good a particular state or state-action pair in terms of expected return. T


\section{Reinforcement Learning for Continuous Space}


\section{Actor-Critic Structure} \label{sec:actor}
