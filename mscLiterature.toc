\contentsline {chapter}{\hbox to\@tempdima {\hfil }Preface}{ix}{chapter*.10}
\contentsline {chapter}{\hbox to\@tempdima {\hfil }Acknowledgements}{xi}{chapter*.11}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1-1}Problem Definition}{2}{section.1.1}
\contentsline {section}{\numberline {1-2}Goal of the Thesis}{2}{section.1.2}
\contentsline {section}{\numberline {1-3}Literature Study Approach}{3}{section.1.3}
\contentsline {section}{\numberline {1-4}Outline}{3}{section.1.4}
\contentsline {chapter}{\numberline {2}Reinforcement Learning Preliminaries}{5}{chapter.2}
\contentsline {section}{\numberline {2-1}Goal as Cost Minimization}{5}{section.2.1}
\contentsline {section}{\numberline {2-2}Markov Decision Process}{6}{section.2.2}
\contentsline {section}{\numberline {2-3}Value Function}{6}{section.2.3}
\contentsline {section}{\numberline {2-4}Policy and value iteration}{7}{section.2.4}
\contentsline {section}{\numberline {2-5}Actor Critic Methods}{7}{section.2.5}
\contentsline {chapter}{\numberline {3}Reinforcement Learning for Tracking Problem: A Survey}{11}{chapter.3}
\contentsline {section}{\numberline {3-1}Reinforcement Learning for Optimal Tracking Control}{11}{section.3.1}
\contentsline {subsection}{\numberline {3-1-1}Standard LQT problem}{12}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3-1-2}Causal Representation of the LQT}{13}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3-1-3}\ac {RL} for Solving the LQT ARE}{14}{subsection.3.1.3}
\contentsline {subsection}{\numberline {3-1-4}\ac {RL} for \ac {LQT} with unknown system dynamics}{15}{subsection.3.1.4}
\contentsline {section}{\numberline {3-2}Dynamic Tuning via Reinforcement Learning}{16}{section.3.2}
\contentsline {subsection}{\numberline {3-2-1}Direct Tuning of Nominal Controller}{17}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3-2-2}Gain scheduling with \ac {PI$^2$}}{18}{subsection.3.2.2}
\contentsline {section}{\numberline {3-3}Nonlinear Input Compensation via Reinforcement Learning}{21}{section.3.3}
\contentsline {subsection}{\numberline {3-3-1}Actor-critic formulation}{21}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3-3-2}\ac {LLR} Function Approximator}{22}{subsection.3.3.2}
\contentsline {chapter}{\numberline {4}Research Direction and Discussion}{25}{chapter.4}
\contentsline {section}{\numberline {4-1}Review of Analysis of \ac {RL} Approaches}{25}{section.4.1}
\contentsline {section}{\numberline {4-2}Simulation Result and Analysis}{25}{section.4.2}
\contentsline {section}{\numberline {4-3}Discussion}{25}{section.4.3}
\contentsline {chapter}{\numberline {5}Future Work and Experiments Plan}{27}{chapter.5}
\contentsline {chapter}{\numberline {6}Conclusion}{29}{chapter.6}
\contentsline {chapter}{\numberline {A}Appendix}{31}{appendix.A}
\contentsline {section}{\numberline {A-1}Simulation Program}{31}{section.A.1}
\contentsline {subsection}{\numberline {A-1-1}A MATLAB listing}{31}{subsection.A.1.1}
\contentsline {chapter}{\hbox to\@tempdima {\hfil }Glossary}{37}{appendix*.52}
\contentsline {section}{\hbox to\@tempdima {\hfil }List of Acronyms}{37}{section*.53}
\contentsline {section}{\hbox to\@tempdima {\hfil }List of Symbols}{38}{section*.55}
