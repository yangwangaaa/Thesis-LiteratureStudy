\BOOKMARK [-1][]{frontmatter_anchor.-1}{Front Matter}{}% 1
\BOOKMARK [0][-]{cover_anchor.0}{Cover Page}{frontmatter_anchor.-1}% 2
\BOOKMARK [0][-]{title_anchor.0}{Title Page}{frontmatter_anchor.-1}% 3
\BOOKMARK [0][-]{toc_anchor.0}{Table of Contents}{frontmatter_anchor.-1}% 4
\BOOKMARK [0][-]{lof_anchor.0}{List of Figures}{frontmatter_anchor.-1}% 5
\BOOKMARK [0][-]{lot_anchor.0}{List of Tables}{frontmatter_anchor.-1}% 6
\BOOKMARK [0][-]{chapter*.8}{Preface}{frontmatter_anchor.-1}% 7
\BOOKMARK [0][-]{chapter*.9}{Acknowledgements}{frontmatter_anchor.-1}% 8
\BOOKMARK [-1][]{mainmatter_anchor.-1}{Main Matter}{}% 9
\BOOKMARK [0][-]{chapter.1}{Introduction}{mainmatter_anchor.-1}% 10
\BOOKMARK [1][-]{section.1.1}{Problem Definition}{chapter.1}% 11
\BOOKMARK [1][-]{section.1.2}{Goal of the Thesis}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.3}{Literature Study Approach}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.4}{Outline}{chapter.1}% 14
\BOOKMARK [0][-]{chapter.2}{Reinforcement Learning Preliminaries}{mainmatter_anchor.-1}% 15
\BOOKMARK [1][-]{section.2.1}{Goal as Cost Minimization}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.2}{Markov Decision Process}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.3}{Value Function}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.4}{Policy and value iteration}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.5}{Actor Critic Methods}{chapter.2}% 20
\BOOKMARK [0][-]{chapter.3}{Reinforcement Learning for Tracking Problem: A Survey}{mainmatter_anchor.-1}% 21
\BOOKMARK [1][-]{section.3.1}{Reinforcement Learning for Optimal Tracking Control}{chapter.3}% 22
\BOOKMARK [2][-]{subsection.3.1.1}{Standard LQT problem}{section.3.1}% 23
\BOOKMARK [2][-]{subsection.3.1.2}{Causal Representation of the LQT}{section.3.1}% 24
\BOOKMARK [2][-]{subsection.3.1.3}{RL for Solving the LQT ARE}{section.3.1}% 25
\BOOKMARK [2][-]{subsection.3.1.4}{Conclusion}{section.3.1}% 26
\BOOKMARK [1][-]{section.3.2}{Dynamic Tuning via Reinforcement Learning}{chapter.3}% 27
\BOOKMARK [2][-]{subsection.3.2.1}{Direct Tuning of Nominal Controller}{section.3.2}% 28
\BOOKMARK [2][-]{subsection.3.2.2}{Gain scheduling with PI2}{section.3.2}% 29
\BOOKMARK [1][-]{section.3.3}{Nonlinear Input Compensation via Reinforcement Learning}{chapter.3}% 30
\BOOKMARK [2][-]{subsection.3.3.1}{Actor-critic formulation}{section.3.3}% 31
\BOOKMARK [2][-]{subsection.3.3.2}{LLR Function Approximator}{section.3.3}% 32
\BOOKMARK [0][-]{chapter.4}{Simulation \046 Verification}{mainmatter_anchor.-1}% 33
\BOOKMARK [1][-]{section.4.1}{Simulated Setup}{chapter.4}% 34
\BOOKMARK [1][-]{section.4.2}{Simulation Result and Analysis}{chapter.4}% 35
\BOOKMARK [1][-]{section.4.3}{Discussion}{chapter.4}% 36
\BOOKMARK [0][-]{chapter.5}{Future Work and Experiments Plan}{mainmatter_anchor.-1}% 37
\BOOKMARK [0][-]{chapter.6}{Conclusion}{mainmatter_anchor.-1}% 38
\BOOKMARK [-1][]{apdx_anchor.-1}{Appendices}{}% 39
\BOOKMARK [0][-]{appendix.A}{Appendix}{apdx_anchor.-1}% 40
\BOOKMARK [1][-]{section.A.1}{Simulation Program}{appendix.A}% 41
\BOOKMARK [2][-]{subsection.A.1.1}{A MATLAB listing}{section.A.1}% 42
\BOOKMARK [-1][]{backmatter_anchor.-1}{Back Matter}{}% 43
\BOOKMARK [0][-]{appendix*.44}{Glossary}{backmatter_anchor.-1}% 44
\BOOKMARK [1][-]{section*.45}{List of Acronyms}{appendix*.44}% 45
\BOOKMARK [1][-]{section*.47}{List of Symbols}{appendix*.44}% 46
