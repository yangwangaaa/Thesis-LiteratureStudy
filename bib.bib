@ARTICLE{6096441,
	author={Grondman, I. and Vaandrager, M. and Busoniu, L. and Babuska, R. and Schuitema, E.},
	journal={Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on},
	title={Efficient Model Learning Methods for Actor #x2013;Critic Control},
	year={2012},
	month={June},
	volume={42},
	number={3},
	pages={591-602},
	keywords={control engineering computing;function approximation;learning (artificial intelligence);manipulators;pendulums;position control;regression analysis;actor-critic algorithm;actor-critic control;function approximation;learning policy update;local linear regression;model learning method;model-based update rule;pendulum swing-up problem;reference model learning;reinforcement learning;robotic arm;Approximation algorithms;Encoding;Function approximation;Process control;Actorâ€“critic;inverse model;local linear regression (LLR);machine learning algorithms;reinforcement learning (RL);Algorithms;Artificial Intelligence;Computer Simulation;Decision Support Techniques;Models, Theoretical;Pattern Recognition, Automated},
	doi={10.1109/TSMCB.2011.2170565},
	ISSN={1083-4419},}

@INPROCEEDINGS{6760476,
	author={Kiumarsi-Khomartash, B. and Lewis, F.L. and Naghibi-Sistani, M.-B. and Karimpour, A.},
	booktitle={Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on},
	title={Optimal tracking control for linear discrete-time systems using reinforcement learning},
	year={2013},
	month={Dec},
	pages={3845-3850},
	keywords={Riccati equations;discrete time systems;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;neural nets;ARE;LQT;actor-critic structure;augmented algebraic Riccati equation;command generator dynamics;drift system dynamics;infinite-horizon linear quadratic tracker;linear discrete-time systems;neural networks;optimal tracking control;policy iteration;reinforcement learning;Approximation methods;Artificial neural networks;Electronic mail;Facsimile;Generators;Riccati equations;Vectors;algebraic Riccati equation;linear quadratic tracker;policy iteration;reinforcement learning},
	doi={10.1109/CDC.2013.6760476},
	ISSN={0743-1546},}


@INPROCEEDINGS{6145931,
	author={Bayiz, Y. E. and Babuska, R.},
	booktitle={International Federation of Aumatomatic Control 2014},
	title={Nonlinear Disturbance Compensation and Reference Tracking via Reinforcement Learning with Fuzzy Approximators},
	year={2014},
	month={August},
	pages={50-54},
	keywords={collision avoidance;learning (artificial intelligence);mobile robots;autonomous navigation;autonomous robot navigation;behavior based architecture;behavior coordination;learning mechanism;obstacle avoidance behavior;physical robot;reinforcement learning method;robot learning;subsumption architecture;Collision avoidance;Computer architecture;Learning;Learning systems;Navigation;Robot kinematics;Q learning;autonomous navigation;behavior coordination;physical robot},
	doi={10.1109/URAI.2011.6145931},}

@book{Sutton1998,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Introduction to Reinforcement Learning},
	year = {1998},
	isbn = {0262193981},
	edition = {1st},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Lewis,
	author={H. Modares and F.L. Lewis},
	booktitle={Automatica},
	title={Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning},
	year={2014},
	pages={1780-1792},
	keywords={Riccati equations;discrete time systems;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;neural nets;ARE;LQT;actor-critic structure;augmented algebraic Riccati equation;command generator dynamics;drift system dynamics;infinite-horizon linear quadratic tracker;linear discrete-time systems;neural networks;optimal tracking control;policy iteration;reinforcement learning;Approximation methods;Artificial neural networks;Electronic mail;Facsimile;Generators;Riccati equations;Vectors;algebraic Riccati equation;linear quadratic tracker;policy iteration;reinforcement learning},
	doi={10.1109/CDC.2013.6760476},
	ISSN={0743-154},}

@ARTICLE{6883207,
	author={Sprangers, O. and Babuska, R. and Nageshrao, S.P. and Lopes, G.A.D.},
	journal={Cybernetics, IEEE Transactions on},
	title={Reinforcement Learning for Port-Hamiltonian Systems},
	year={2014},
	month={},
	volume={PP},
	number={99},
	pages={1-1},
	keywords={Damping;Equations;Learning (artificial intelligence);Mathematical model;Optimal control;Stability analysis;Vectors;Actor-critic (AC);energy-balancing (EB);passivity-based control (PBC);port-Hamiltonian (PH) systems;reinforcement learning (RL)},
	doi={10.1109/TCYB.2014.2343194},
	ISSN={2168-2267},}

@book{RussTedrake,
	author = {Russ Tedrake},
	title = {Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832)},
	year = {2014}, 	
	isbn = {02621939812},
	publisher = {MIT},}
\\
