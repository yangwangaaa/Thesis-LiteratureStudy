@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={28},
  year={1998},
  publisher={MIT press}
}


@INPROCEEDINGS{einstein,
  title={On the Method of Theoretical Physics},
  author={Albert Einstein},
  volume={1},
  year={1934},
  pages={163-169}, 
  publisher={Philosophy of Science}
}

@INPROCEEDINGS{Brujeni5669655, 
author={Brujeni, L.A. and Jong Min Lee and Shah, S.L.}, 
booktitle={Control Automation and Systems (ICCAS), 2010 International Conference on}, 
title={Dynamic tuning of PI-controllers based on model-free Reinforcement Learning methods}, 
year={2010}, 
month={Oct}, 
pages={453-458}, 
keywords={PI control;chemical engineering;control engineering computing;delays;
learning (artificial intelligence);state feedback;CSTH;FOPTD;Pi-controllers;RL;
continuous stirred tank heater;dynamic tuning;feedback controllers;
first order plus time delay;model free reinforcement learning methods;
plant model mismatch;Adaptation model;Chemicals;Process control;Simulation;
Temperature control;Training;Tuning;Dynamic Tuning;PI-Controller;
Reinforcement Learning;SARSA;Self-Tuning},}

@ARTICLE{Grondman6096441, 
	author={Grondman, I. and Vaandrager, M. and Busoniu, L. and Babuska, R. and Schuitema, E.}, 
	journal={Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on}, 
	title={Efficient Model Learning Methods for Actor Critic Control}, 
	year={2012}, 
	month={June}, 
	volume={42}, 
	number={3}, 
	pages={591-602}, 
	keywords={control engineering computing;function approximation;learning (artificial intelligence);manipulators;pendulums;position control;regression analysis;actor-critic algorithm;actor-critic control;function approximation;learning policy update;local linear regression;model learning method;model-based update rule;pendulum swing-up problem;reference model learning;reinforcement learning;robotic arm;Approximation algorithms;Encoding;Function approximation;Process control;Actor–critic;inverse model;local linear regression (LLR);machine learning algorithms;reinforcement learning (RL);Algorithms;Artificial Intelligence;Computer Simulation;Decision Support Techniques;Models, Theoretical;Pattern Recognition, Automated}, 
	doi={10.1109/TSMCB.2011.2170565}, 
	ISSN={1083-4419},}

@INPROCEEDINGS{Modares6760477, 
	author={Modares, H. and Lewis, F.L.}, 
	booktitle={Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on}, 
	title={Online solution to the linear quadratic tracking problem of continuous-time systems using reinforcement learning}, 
	year={2013}, 
	month={Dec}, 
	pages={3851-3856}, 
	keywords={Riccati equations;continuous time systems;feedforward;learning systems;linear systems;optimal control;LQ regulator;LQ tracker;LQT;RL techniques;algebraic Riccati equation;augmented system;command generator dynamics;continuous-time systems;feedforward term;linear quadratic tracking problem;noncausality problem;optimal control solution;reinforcement learning;tracker control;Gold}, 
	doi={10.1109/CDC.2013.6760477}, 
	ISSN={0743-1546},}

@ARTICLE{Lewis5227780, 
	author={Lewis, F.L. and Vrabie, D.}, 
	journal={Circuits and Systems Magazine, IEEE}, 
	title={Reinforcement learning and adaptive dynamic programming for feedback control}, 
	year={2009}, 
	month={Third}, 
	volume={9}, 
	number={3}, 
	pages={32-50}, 
	keywords={control engineering computing;control system synthesis;dynamic programming;feedback;learning (artificial intelligence);optimal control;adaptive dynamic programming;controller design;feedback control;man-made engineered system;natural system;optimal behavior;reinforcement learning;reward stimulus;Adaptive control;Control systems;Design engineering;Dynamic programming;Feedback control;Learning;Optimal control;Organisms;Programmable control;Systems engineering and theory}, 
	doi={10.1109/MCAS.2009.933854}, 
	ISSN={1531-636X},}

@INPROCEEDINGS{Buchli6037312, 
	author={Stulp, F. and Buchli, J. and Ellmer, A. and Mistry, M. and Theodorou, E. and Schaal, S.}, 
	booktitle={Development and Learning (ICDL), 2011 IEEE International Conference on}, 
	title={Reinforcement learning of impedance control in stochastic force fields}, 
	year={2011}, 
	month={Aug}, 
	volume={2}, 
	pages={1-6}, 
	keywords={end effectors;feedforward;learning systems;stochastic systems;biologically plausible approach;end-effector trajectories;external perturbations;feedforward command;impedance perturbations;model-free reinforcement learning algorithm;physical interaction;robot end-effector;simulated 7-DOF robot;stochastic force fields;two-fold strategy;variable impedance control;Computational modeling;Noise measurement}, 
	doi={10.1109/DEVLRN.2011.6037312}, 
	ISSN={2161-9476},}

@INPROCEEDINGS{Kiumarsi6760476, 
	author={Kiumarsi-Khomartash, B. and Lewis, F.L. and Naghibi-Sistani, M.-B. and Karimpour, A.}, 
	booktitle={Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on}, 
	title={Optimal tracking control for linear discrete-time systems using reinforcement learning}, 
	year={2013}, 
	month={Dec}, 
	pages={3845-3850}, 
	keywords={Riccati equations;discrete time systems;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;neural nets;ARE;LQT;actor-critic structure;augmented algebraic Riccati equation;command generator dynamics;drift system dynamics;infinite-horizon linear quadratic tracker;linear discrete-time systems;neural networks;optimal tracking control;policy iteration;reinforcement learning;Approximation methods;Artificial neural networks;Electronic mail;Facsimile;Generators;Riccati equations;Vectors;algebraic Riccati equation;linear quadratic tracker;policy iteration;reinforcement learning}, 
	doi={10.1109/CDC.2013.6760476}, 
	ISSN={0743-1546},}

@article{Modares20141780,
	title = "Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning ",
	journal = "Automatica ",
	volume = "50",
	number = "7",
	pages = "1780 - 1792",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.05.011",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814001861",
	author = "Hamidreza Modares and Frank L. Lewis",
	keywords = "Optimal tracking control",
	keywords = "Integral reinforcement learning",
	keywords = "Input constrainers",
	keywords = "Neural networks "
}

@ARTICLE{Kiumarsi6918527, 
	author={Kiumarsi, B. and Lewis, F.L.}, 
	journal={Neural Networks and Learning Systems, IEEE Transactions on}, 
	title={Actor-Critic-Based Optimal Tracking for Partially Unknown Nonlinear Discrete-Time Systems}, 
	year={2014}, 
	month={}, 
	volume={PP}, 
	number={99}, 
	pages={1-1}, 
	keywords={Equations;Feedforward neural networks;Heuristic algorithms;Mathematical model;Nonlinear dynamical systems;Standards;Trajectory;Actor–critic algorithm;discrete-time (DT) nonlinear optimal tracking;input constraints;neural network (NN);reinforcement learning (RL)}, 
	doi={10.1109/TNNLS.2014.2358227}, 
	ISSN={2162-237X},}

@article{Kiumarsi20141167,
	title = "Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics ",
	journal = "Automatica ",
	volume = "50",
	number = "4",
	pages = "1167 - 1175",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Bahare Kiumarsi and Frank L. Lewis and Hamidreza Modares and Ali Karimpour and Mohammad-Bagher Naghibi-Sistani",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}


@article{Buchli2010,
	title = "Variable Impedance Control-A Reinforcement Learning Approach",
	journal = "Robotics: Science and Systems",
	year = "2010",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Jonas Buchli and Evangelos Theodorou and Freek Stulp and Stefan Schaal",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}


@article{Efe2014,
	title = "Nonlinear Disturbance Compensation and. Reference Tracking via Reinforcement. Learning with Fuzzy Approximators",
	journal = "19th IFAC World Congress",
	year = "2010",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Y. Efe Bayiz and Robert Babuska",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}

@article{IFR2014,
	title = "Industrial Robot Statistics",
	journal = "World Robotics 2014 Industrial Robots",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "International Federation of Robotics (IFR)",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}

@article{Lloret2014,
	title = "Complex concrete structures: Merging existing casting techniques with digital fabrication",
	journal = "Computer-Aided Design",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Ena Lloret and Amir R. Shahabb and Mettler Linus and Robert J. Flatt and Fabio Gramazio and Matthias Kohler and Silke Langenberg",
	keywords = "Complex concrete structures"
}

@article{Helm2014,
	title = "In-Situ Robotic Fabrication: Advanced Digital Manufacturing Beyond the Laboratory",
	journal = "Springer Tracts in Advanced Robotics 2014",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Volker Helm and Jan Willmann and Fabio Gramazio and Matthias Kohler",
	keywords = "In-situ",
	keywords = "automated construction",
	keywords = "additive fabrication"
}

@article{babuskaRL,
  title={SC4081 Knowledge-based Control Systems lecture slide},
  author={Robert Babuska},
  publisher={Delft Center for Systems & Control, TU Delft, The Netherlands}
}

@incollection{Bertsekas,
year={2007},
isbn={978-3-540-69994-1},
booktitle={Operations Research Proceedings 2006},
volume={2006},
series={Operations Research Proceedings},
editor={Waldmann, Karl-Heinz and Stocker, UlrikeM.},
doi={10.1007/978-3-540-69995-8_11},
title={Neuro-Dynamic Programming: An Overview and Recent Results},
url={http://dx.doi.org/10.1007/978-3-540-69995-8_11},
publisher={Springer Berlin Heidelberg},
author={Bertsekas, DimitriP.},
pages={71-72},
language={English}
}

@incollection{Abbeel,
year={2010},
isbn={978-0-387-30768-8},
booktitle={Encyclopedia of Machine Learning},
editor={Sammut, Claude and Webb, GeoffreyI.},
doi={10.1007/978-0-387-30164-8_45},
title={Autonomous Helicopter Flight Using Reinforcement Learning},
url={http://dx.doi.org/10.1007/978-0-387-30164-8_45},
publisher={Springer US},
author={Coates, Adam and Abbeel, Pieter and Ng, AndrewY.},
pages={53-61},
language={English}
}

@incollection{NIPS2007_3253,
title = {Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion},
author = {J. Z. Kolter and Abbeel, Pieter and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {J.C. Platt and D. Koller and Y. Singer and S.T. Roweis},
pages = {769--776},
year = {2008},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3253-hierarchical-apprenticeship-learning-with-application-to-quadruped-locomotion.pdf}
}

@INPROCEEDINGS{4543641,
author={Ross, S. and Chaib-draa, B. and Pineau, J.},
booktitle={Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
title={Bayesian reinforcement learning in continuous POMDPs with application to robot navigation},
year={2008},
month={May},
pages={2845-2851},
keywords={Markov processes;belief networks;learning (artificial intelligence);optimal control;particle filtering (numerical methods);path planning;position control;robots;Bayesian reinforcement learning;continuous POMDP;observable Markov decision processes;online planning algorithm;optimal control;particle filter algorithm;robot navigation;trajectory sampling;Bayesian methods;Learning;Mathematical model;Motion planning;Navigation;Optimal control;Particle filters;Robot sensing systems;Sampling methods;Trajectory},
doi={10.1109/ROBOT.2008.4543641},
ISSN={1050-4729},}

@ARTICLE{126844,
author={Sutton, R.S. and Barto, A.G. and Williams, Ronald J.},
journal={Control Systems, IEEE},
title={Reinforcement learning is direct adaptive optimal control},
year={1992},
month={April},
volume={12},
number={2},
pages={19-22},
keywords={adaptive control;approximation theory;dynamic programming;learning systems;neural nets;nonlinear control systems;optimal control;Q-learning systems;direct adaptive optimal control;dynamic programming;hybrid direct/indirect methods;neural network reinforcement learning;nonlinear systems;state-action pair estimates;stochastic approximation;Adaptive control;Animals;Control system synthesis;Dynamic programming;Learning;Neural networks;Nonlinear systems;Optimal control;Programmable control;State estimation},
doi={10.1109/37.126844},
ISSN={1066-033X},}

@INPROCEEDINGS{4048052,
author={Arimoto, S. and Kawamura, S. and Miyazaki, F.},
booktitle={Decision and Control, 1984. The 23rd IEEE Conference on},
title={Bettering operation of dynamic systems by learning: A new control theory for servomechanism or mechatronics systems},
year={1984},
month={Dec},
pages={1064-1069},
keywords={Control systems;Control theory;Mechatronics;Servomechanisms},
doi={10.1109/CDC.1984.272176},}

@ARTICLE{1044377,
author={Norrlof, M. and Gunnarsson, S.},
journal={Robotics and Automation, IEEE Transactions on},
title={Experimental comparison of some classical iterative learning control algorithms},
year={2002},
month={Aug},
volume={18},
number={4},
pages={636-641},
keywords={industrial robots;matrix algebra;robust control;stability;classical iterative learning control algorithms;commercial industrial robot;industrial robot;model-based algorithms;Actuators;Algorithm design and analysis;Control systems;Electrical equipment industry;History;Industrial control;Iterative algorithms;Iterative methods;Robot control;Service robots},
doi={10.1109/TRA.2002.802210},
ISSN={1042-296X},}

@INPROCEEDINGS{299157,
author={Kim, D.I. and Sungkwun Kim},
booktitle={Industry Applications Society Annual Meeting, 1993., Conference Record of the 1993 IEEE},
title={An iterative learning control method with application for CNC machine tools},
year={1993},
month={Oct},
pages={2106-2111 vol.3},
keywords={adaptive control;computerised numerical control;industrial robots;learning by example;machine tools;manufacturing computer control;three-term control;CNC machine tools;PID control;accuracy;algorithm;application;convergence;industrial robots;iterative learning control method;output error;performance;periodic disturbances;precise tracking control;workpiece;Computer numerical control;Electrical equipment industry;Iterative methods;Machine learning;Machine tools;Machining;PD control;Pi control;Proportional control;Three-term control},
doi={10.1109/IAS.1993.299157},}

@book{lewis1995optimal,
  title={Optimal control},
  author={Lewis, Frank L and Syrmos, Vassilis L},
  year={1995},
  publisher={John Wiley \& Sons}
}

@ARTICLE{1099755,
author={Hewer, G.},
journal={Automatic Control, IEEE Transactions on},
title={An iterative technique for the computation of the steady state gains for the discrete optimal regulator},
year={1971},
month={Aug},
volume={16},
number={4},
pages={382-384},
keywords={Linear systems, time-invariant discrete-time;Numerical methods;Optimal regulators;Algebra;Electrons;Equations;Iterative methods;Linear systems;Optimal control;Regulators;Steady-state;System analysis and design;Writing},
doi={10.1109/TAC.1971.1099755},
ISSN={0018-9286},}

@article{buchli2011learning,
  title={Learning variable impedance control},
  author={Buchli, Jonas and Stulp, Freek and Theodorou, Evangelos and Schaal, Stefan},
  journal={The International Journal of Robotics Research},
  volume={30},
  number={7},
  pages={820--833},
  year={2011},
  publisher={SAGE Publications}
}

@article{theodorou2010generalized,
  title={A generalized path integral control approach to reinforcement learning},
  author={Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={3137--3181},
  year={2010},
  publisher={JMLR. org}
}

