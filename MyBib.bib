@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={28},
  year={1998},
  publisher={MIT press}
}

@INPROCEEDINGS{einstein,
  title={On the Method of Theoretical Physics},
  author={Albert Einstein},
  volume={1},
  year={1934},
  pages={163-169}, 
  publisher={Philosophy of Science}
}

@INPROCEEDINGS{Brujeni5669655, 
author={Brujeni, L.A. and Jong Min Lee and Shah, S.L.}, 
booktitle={Control Automation and Systems (ICCAS), 2010 International Conference on}, 
title={Dynamic tuning of PI-controllers based on model-free Reinforcement Learning methods}, 
year={2010}, 
month={Oct}, 
pages={453-458}, 
keywords={PI control;chemical engineering;control engineering computing;delays;
learning (artificial intelligence);state feedback;CSTH;FOPTD;Pi-controllers;RL;
continuous stirred tank heater;dynamic tuning;feedback controllers;
first order plus time delay;model free reinforcement learning methods;
plant model mismatch;Adaptation model;Chemicals;Process control;Simulation;
Temperature control;Training;Tuning;Dynamic Tuning;PI-Controller;
Reinforcement Learning;SARSA;Self-Tuning},}

@ARTICLE{Grondman6096441, 
	author={Grondman, I. and Vaandrager, M. and Busoniu, L. and Babuska, R. and Schuitema, E.}, 
	journal={Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on}, 
	title={Efficient Model Learning Methods for Actor Critic Control}, 
	year={2012}, 
	month={June}, 
	volume={42}, 
	number={3}, 
	pages={591-602}, 
	keywords={control engineering computing;function approximation;learning (artificial intelligence);manipulators;pendulums;position control;regression analysis;actor-critic algorithm;actor-critic control;function approximation;learning policy update;local linear regression;model learning method;model-based update rule;pendulum swing-up problem;reference model learning;reinforcement learning;robotic arm;Approximation algorithms;Encoding;Function approximation;Process control;Actor–critic;inverse model;local linear regression (LLR);machine learning algorithms;reinforcement learning (RL);Algorithms;Artificial Intelligence;Computer Simulation;Decision Support Techniques;Models, Theoretical;Pattern Recognition, Automated}, 
	doi={10.1109/TSMCB.2011.2170565}, 
	ISSN={1083-4419},}

@INPROCEEDINGS{Modares6760477, 
	author={Modares, H. and Lewis, F.L.}, 
	booktitle={Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on}, 
	title={Online solution to the linear quadratic tracking problem of continuous-time systems using reinforcement learning}, 
	year={2013}, 
	month={Dec}, 
	pages={3851-3856}, 
	keywords={Riccati equations;continuous time systems;feedforward;learning systems;linear systems;optimal control;LQ regulator;LQ tracker;LQT;RL techniques;algebraic Riccati equation;augmented system;command generator dynamics;continuous-time systems;feedforward term;linear quadratic tracking problem;noncausality problem;optimal control solution;reinforcement learning;tracker control;Gold}, 
	doi={10.1109/CDC.2013.6760477}, 
	ISSN={0743-1546},}

@ARTICLE{Lewis5227780, 
	author={Lewis, F.L. and Vrabie, D.}, 
	journal={Circuits and Systems Magazine, IEEE}, 
	title={Reinforcement learning and adaptive dynamic programming for feedback control}, 
	year={2009}, 
	month={Third}, 
	volume={9}, 
	number={3}, 
	pages={32-50}, 
	keywords={control engineering computing;control system synthesis;dynamic programming;feedback;learning (artificial intelligence);optimal control;adaptive dynamic programming;controller design;feedback control;man-made engineered system;natural system;optimal behavior;reinforcement learning;reward stimulus;Adaptive control;Control systems;Design engineering;Dynamic programming;Feedback control;Learning;Optimal control;Organisms;Programmable control;Systems engineering and theory}, 
	doi={10.1109/MCAS.2009.933854}, 
	ISSN={1531-636X},}

@INPROCEEDINGS{Buchli6037312, 
	author={Stulp, F. and Buchli, J. and Ellmer, A. and Mistry, M. and Theodorou, E. and Schaal, S.}, 
	booktitle={Development and Learning (ICDL), 2011 IEEE International Conference on}, 
	title={Reinforcement learning of impedance control in stochastic force fields}, 
	year={2011}, 
	month={Aug}, 
	volume={2}, 
	pages={1-6}, 
	keywords={end effectors;feedforward;learning systems;stochastic systems;biologically plausible approach;end-effector trajectories;external perturbations;feedforward command;impedance perturbations;model-free reinforcement learning algorithm;physical interaction;robot end-effector;simulated 7-DOF robot;stochastic force fields;two-fold strategy;variable impedance control;Computational modeling;Noise measurement}, 
	doi={10.1109/DEVLRN.2011.6037312}, 
	ISSN={2161-9476},}

@INPROCEEDINGS{Kiumarsi6760476, 
	author={Kiumarsi-Khomartash, B. and Lewis, F.L. and Naghibi-Sistani, M.-B. and Karimpour, A.}, 
	booktitle={Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on}, 
	title={Optimal tracking control for linear discrete-time systems using reinforcement learning}, 
	year={2013}, 
	month={Dec}, 
	pages={3845-3850}, 
	keywords={Riccati equations;discrete time systems;iterative methods;learning (artificial intelligence);linear quadratic control;linear systems;neural nets;ARE;LQT;actor-critic structure;augmented algebraic Riccati equation;command generator dynamics;drift system dynamics;infinite-horizon linear quadratic tracker;linear discrete-time systems;neural networks;optimal tracking control;policy iteration;reinforcement learning;Approximation methods;Artificial neural networks;Electronic mail;Facsimile;Generators;Riccati equations;Vectors;algebraic Riccati equation;linear quadratic tracker;policy iteration;reinforcement learning}, 
	doi={10.1109/CDC.2013.6760476}, 
	ISSN={0743-1546},}

@article{Modares20141780,
	title = "Optimal tracking control of nonlinear partially-unknown constrained-input systems using integral reinforcement learning ",
	journal = "Automatica ",
	volume = "50",
	number = "7",
	pages = "1780 - 1792",
	year = "2014",
	note = "",
	author = "Hamidreza Modares and Frank L. Lewis",
	keywords = "Optimal tracking control",
	keywords = "Integral reinforcement learning",
	keywords = "Input constrainers",
	keywords = "Neural networks "
}

@ARTICLE{Kiumarsi6918527, 
	author={Kiumarsi, B. and Lewis, F.L.}, 
	journal={Neural Networks and Learning Systems, IEEE Transactions on}, 
	title={Actor-Critic-Based Optimal Tracking for Partially Unknown Nonlinear Discrete-Time Systems}, 
	year={2014}, 
	month={}, 
	volume={PP}, 
	number={99}, 
	pages={1-1}, 
	keywords={Equations;Feedforward neural networks;Heuristic algorithms;Mathematical model;Nonlinear dynamical systems;Standards;Trajectory;Actor–critic algorithm;discrete-time (DT) nonlinear optimal tracking;input constraints;neural network (NN);reinforcement learning (RL)}, }

@article{Kiumarsi20141167,
	title = "Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics ",
	journal = "Automatica ",
	volume = "50",
	number = "4",
	pages = "1167 - 1175",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Bahare Kiumarsi and Frank L. Lewis and Hamidreza Modares and Ali Karimpour and Mohammad-Bagher Naghibi-Sistani",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}


@article{Buchli2010,
	title = "Variable Impedance Control-A Reinforcement Learning Approach",
	journal = "Robotics: Science and Systems",
	year = "2010",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Jonas Buchli and Evangelos Theodorou and Freek Stulp and Stefan Schaal",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}


@article{Efe2014,
	title = "Nonlinear Disturbance Compensation and. Reference Tracking via Reinforcement. Learning with Fuzzy Approximators",
	journal = "19th IFAC World Congress",
	year = "2014",
	note = "",
	author = "Y. Efe Bayiz and Robert Babuska",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}

@article{IFR2014,
	title = "Industrial Robot Statistics",
	journal = "World Robotics 2014 Industrial Robots",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "International Federation of Robotics (IFR)",
	keywords = "Linear quadratic tracker",
	keywords = "Reinforcement learning",
	keywords = "Policy iteration",
	keywords = "Algebraic Riccati equation "
}

@article{Lloret2014,
	title = "Complex concrete structures: Merging existing casting techniques with digital fabrication",
	journal = "Computer-Aided Design",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Ena Lloret and Amir R. Shahabb and Mettler Linus and Robert J. Flatt and Fabio Gramazio and Matthias Kohler and Silke Langenberg",
	keywords = "Complex concrete structures"
}

@article{Helm2014,
	title = "In-Situ Robotic Fabrication: Advanced Digital Manufacturing Beyond the Laboratory",
	journal = "Springer Tracts in Advanced Robotics 2014",
	year = "2014",
	note = "",
	issn = "0005-1098",
	doi = "http://dx.doi.org/10.1016/j.automatica.2014.02.015",
	url = "http://www.sciencedirect.com/science/article/pii/S0005109814000533",
	author = "Volker Helm and Jan Willmann and Fabio Gramazio and Matthias Kohler",
	keywords = "In-situ",
	keywords = "automated construction",
	keywords = "additive fabrication"
}

@article{babuskaRL,
  title={SC4081 Knowledge-based Control Systems lecture slide},
  author={Robert Babuska},
  publisher={Delft Center for Systems & Control, TU Delft, The Netherlands}
}

@incollection{Bertsekas,
year={2007},
isbn={978-3-540-69994-1},
booktitle={Operations Research Proceedings 2006},
volume={2006},
series={Operations Research Proceedings},
editor={Waldmann, Karl-Heinz and Stocker, UlrikeM.},
doi={10.1007/978-3-540-69995-8_11},
title={Neuro-Dynamic Programming: An Overview and Recent Results},
url={http://dx.doi.org/10.1007/978-3-540-69995-8_11},
publisher={Springer Berlin Heidelberg},
author={Bertsekas, DimitriP.},
pages={71-72},
language={English}
}

@incollection{Abbeel,
year={2010},
isbn={978-0-387-30768-8},
booktitle={Encyclopedia of Machine Learning},
editor={Sammut, Claude and Webb, GeoffreyI.},
doi={10.1007/978-0-387-30164-8_45},
title={Autonomous Helicopter Flight Using Reinforcement Learning},
url={http://dx.doi.org/10.1007/978-0-387-30164-8_45},
publisher={Springer US},
author={Coates, Adam and Abbeel, Pieter and Ng, AndrewY.},
pages={53-61},
language={English}
}

@incollection{NIPS2007_3253,
title = {Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion},
author = {J. Z. Kolter and Abbeel, Pieter and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {J.C. Platt and D. Koller and Y. Singer and S.T. Roweis},
pages = {769--776},
year = {2008},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3253-hierarchical-apprenticeship-learning-with-application-to-quadruped-locomotion.pdf}
}

@INPROCEEDINGS{4543641,
author={Ross, S. and Chaib-draa, B. and Pineau, J.},
booktitle={Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
title={Bayesian reinforcement learning in continuous POMDPs with application to robot navigation},
year={2008},
month={May},
pages={2845-2851},
keywords={Markov processes;belief networks;learning (artificial intelligence);optimal control;particle filtering (numerical methods);path planning;position control;robots;Bayesian reinforcement learning;continuous POMDP;observable Markov decision processes;online planning algorithm;optimal control;particle filter algorithm;robot navigation;trajectory sampling;Bayesian methods;Learning;Mathematical model;Motion planning;Navigation;Optimal control;Particle filters;Robot sensing systems;Sampling methods;Trajectory},
doi={10.1109/ROBOT.2008.4543641},
ISSN={1050-4729},}

@ARTICLE{126844,
author={Sutton, R.S. and Barto, A.G. and Williams, Ronald J.},
journal={Control Systems, IEEE},
title={Reinforcement learning is direct adaptive optimal control},
year={1992},
month={April},
volume={12},
number={2},
pages={19-22},
keywords={adaptive control;approximation theory;dynamic programming;learning systems;neural nets;nonlinear control systems;optimal control;Q-learning systems;direct adaptive optimal control;dynamic programming;hybrid direct/indirect methods;neural network reinforcement learning;nonlinear systems;state-action pair estimates;stochastic approximation;Adaptive control;Animals;Control system synthesis;Dynamic programming;Learning;Neural networks;Nonlinear systems;Optimal control;Programmable control;State estimation},
doi={10.1109/37.126844},
ISSN={1066-033X},}

@INPROCEEDINGS{4048052,
author={Arimoto, S. and Kawamura, S. and Miyazaki, F.},
booktitle={Decision and Control, 1984. The 23rd IEEE Conference on},
title={Bettering operation of dynamic systems by learning: A new control theory for servomechanism or mechatronics systems},
year={1984},
month={Dec},
pages={1064-1069},
keywords={Control systems;Control theory;Mechatronics;Servomechanisms},
doi={10.1109/CDC.1984.272176},}

@ARTICLE{1044377,
author={Norrlof, M. and Gunnarsson, S.},
journal={Robotics and Automation, IEEE Transactions on},
title={Experimental comparison of some classical iterative learning control algorithms},
year={2002},
month={Aug},
volume={18},
number={4},
pages={636-641},
keywords={industrial robots;matrix algebra;robust control;stability;classical iterative learning control algorithms;commercial industrial robot;industrial robot;model-based algorithms;Actuators;Algorithm design and analysis;Control systems;Electrical equipment industry;History;Industrial control;Iterative algorithms;Iterative methods;Robot control;Service robots},
doi={10.1109/TRA.2002.802210},
ISSN={1042-296X},}

@INPROCEEDINGS{299157,
author={Kim, D.I. and Sungkwun Kim},
booktitle={Industry Applications Society Annual Meeting, 1993., Conference Record of the 1993 IEEE},
title={An iterative learning control method with application for CNC machine tools},
year={1993},
month={Oct},
pages={2106-2111 vol.3},
keywords={adaptive control;computerised numerical control;industrial robots;learning by example;machine tools;manufacturing computer control;three-term control;CNC machine tools;PID control;accuracy;algorithm;application;convergence;industrial robots;iterative learning control method;output error;performance;periodic disturbances;precise tracking control;workpiece;Computer numerical control;Electrical equipment industry;Iterative methods;Machine learning;Machine tools;Machining;PD control;Pi control;Proportional control;Three-term control},
doi={10.1109/IAS.1993.299157},}

@book{lewis1995optimal,
  title={Optimal control},
  author={Lewis, Frank L and Syrmos, Vassilis L},
  year={1995},
  publisher={John Wiley \& Sons}
}

@ARTICLE{1099755,
author={Hewer, G.},
journal={Automatic Control, IEEE Transactions on},
title={An iterative technique for the computation of the steady state gains for the discrete optimal regulator},
year={1971},
month={Aug},
volume={16},
number={4},
pages={382-384},
keywords={Linear systems, time-invariant discrete-time;Numerical methods;Optimal regulators;Algebra;Electrons;Equations;Iterative methods;Linear systems;Optimal control;Regulators;Steady-state;System analysis and design;Writing},
doi={10.1109/TAC.1971.1099755},
ISSN={0018-9286},}

@article{buchli2011learning,
  title={Learning variable impedance control},
  author={Buchli, Jonas and Stulp, Freek and Theodorou, Evangelos and Schaal, Stefan},
  journal={The International Journal of Robotics Research},
  volume={30},
  number={7},
  pages={820--833},
  year={2011},
  publisher={SAGE Publications}
}

@article{theodorou2010generalized,
  title={A generalized path integral control approach to reinforcement learning},
  author={Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={3137--3181},
  year={2010},
  publisher={JMLR. org}
}

@INPROCEEDINGS{375142,
author={Jang, J.-S.R. and Gulley, N.},
booktitle={Fuzzy Information Processing Society Biannual Conference, 1994. Industrial Fuzzy Control and Intelligent Systems Conference, and the NASA Joint Technology Workshop on Neural Networks and Fuzzy Logic,},
title={Gain scheduling based fuzzy controller design},
year={1994},
month={Dec},
pages={101-105},
keywords={control system synthesis;fuzzy control;scheduling;Sugeno fuzzy controller;ball-beam system;cart-pole system;feedback gains;fuzzy controller design;fuzzy if-then rules;gain scheduling;membership functions;Control systems;Design methodology;Expert systems;Feedback;Fuzzy control;Fuzzy sets;Fuzzy systems;Humans;Hybrid intelligent systems;Scheduling},
doi={10.1109/IJCF.1994.375142},}

@INPROCEEDINGS{5229855,
author={Zhaoqin Guo and Jian-Xin Xu and Tong Heng Lee},
booktitle={Advanced Intelligent Mechatronics, 2009. AIM 2009. IEEE/ASME International Conference on},
title={A gain-scheduling optimal fuzzy logic controller design for unicycle},
year={2009},
month={July},
pages={1423-1428},
keywords={control system synthesis;fuzzy control;linear quadratic control;linearisation techniques;matrix algebra;nonlinear control systems;pendulums;scheduling;state feedback;vehicle dynamics;FLC;LQR;gain-scheduling;input-output feedback linearization;inverse pendulum;linear quadratic regulator;operating point;optimal fuzzy logic controller design;saddle;state feedback;torque generation;under-actuated mechanism;unicycle control;unstable equilibrium;weighting matrix;zero dynamics;Actuators;Control systems;Fuzzy logic;Nonlinear control systems;Optimal control;State feedback;State-space methods;Vehicle dynamics;Vehicles;Wheels},
doi={10.1109/AIM.2009.5229855},}

@INPROCEEDINGS{1684589,
author={Hazzab, A. and Bousserhane, I.K. and Zerbo, M. and Sicard, P.},
booktitle={Information and Communication Technologies, 2006. ICTTA '06. 2nd},
title={Real Time Implementation of Fuzzy Gain Scheduling of PI Controller for Induction Machine Control.},
year={2006},
month={},
volume={1},
pages={1416-1421},
keywords={PI control;fuzzy control;induction motors;machine control;PI controller;controller parameters;fuzzy gain scheduling;fuzzy rules;induction machine control;real time implementation;tracking error;Adaptive control;Control nonlinearities;DC machines;Error correction;Fuzzy control;Fuzzy logic;Induction machines;Nonlinear control systems;Programmable control;Three-term control},
doi={10.1109/ICTTA.2006.1684589},}

@INPROCEEDINGS{6606304,
author={Yujin Cheon and Donghyun Lee and In-Beum Lee and Su Whan Sung},
booktitle={Control Conference (ASCC), 2013 9th Asian},
title={A new PID auto-tuning strategy with operational optimization for MCFC systems},
year={2013},
month={June},
pages={1-6},
keywords={molten carbonate fuel cells;neurocontrollers;optimisation;power control;radial basis function networks;reduced order systems;self-adjusting systems;three-term control;MCFC system;PID autotuning strategy;fractional low order model;frequency response identification;model reduction;molten carbonate fuel cell;operational optimization;power demand;radial basis functional neural network model;Frequency response;Fuels;Optimization;Power demand;Relays;Steady-state;Tuning;Fuel Cells;PID control;auto-tuning;gain scheduling;operational optimization},
doi={10.1109/ASCC.2013.6606304},}

@INPROCEEDINGS{572744,
author={Chu, C.-K. and Gwo-Ruey Yu and Jonckheere, E.A. and Youssef, H.M.},
booktitle={Decision and Control, 1996., Proceedings of the 35th IEEE Conference on},
title={Gain scheduling for fly-by-throttle flight control using neural networks},
year={1996},
month={Dec},
volume={2},
pages={1557-1562 vol.2},
keywords={H∞ control;aircraft control;control system synthesis;feedforward neural nets;model reference adaptive control systems;neurocontrollers;H∞ method;L-1011;PCA system;crippled aircraft;emergency fly-by-throttle control;fly-by-throttle flight control;gain scheduling;gain scheduling linear dynamic controllers;linear controller synthesis;model matching problem;neural networks;pitch control design;propulsion controlled aircraft system;radial basis network;Aerospace control;Aircraft propulsion;Control design;Control system synthesis;Control systems;Dynamic scheduling;Gain;Network synthesis;Neural networks;Principal component analysis},
doi={10.1109/CDC.1996.572744},
ISSN={0191-2216},}

@INPROCEEDINGS{556252,
author={Joo-Siong Chai and Shaohua Tan and Chang-Chieh Hang},
booktitle={Intelligent Control, 1996., Proceedings of the 1996 IEEE International Symposium on},
title={Gain scheduling control of nonlinear plant using RBF neural network},
year={1996},
month={Sep},
pages={502-507},
keywords={interpolation;PI control;RBF neural network;gain scheduling control;linear controller;nonlinear plant;online interpolation;partitioning algorithm;Algorithm design and analysis;Computer simulation;Interpolation;Neural networks;Partitioning algorithms;Performance analysis;Performance gain;Pi control;Processor scheduling;Radial basis function networks},
doi={10.1109/ISIC.1996.556252},
ISSN={2158-9860},}

@INPROCEEDINGS{882916,
author={Albertos, P. and Olivares, M.},
booktitle={Intelligent Control, 2000. Proceedings of the 2000 IEEE International Symposium on},
title={Online learning control of a gantry crane},
year={2000},
month={},
pages={157-162},
keywords={adaptive control;cranes;industrial control;learning (artificial intelligence);learning systems;position control;controller structures;disturbances compensation;gantry crane;harbor installations;learning capabilities;load swinging;online learning control;Adaptive control;Automatic control;Control systems;Cranes;Fuzzy control;Fuzzy systems;Industrial control;Learning;Process control;Systems engineering and theory},
doi={10.1109/ISIC.2000.882916},
ISSN={2158-9860},}


@book{stengel1994,
  title={Optimal Control and Estimation},
  author={Stengel, Robert F.},
  year={1994},
  publisher={Dover Publications}
}

@book{oksendal2010,
  title={Stochastic Differential Equations: An Introduction with Applications},
  author={Oksendal, Bernt K.},
  year={2010},
  publisher={Springer}
}

@INPROCEEDINGS{5509336,
author={Theodorou, E. and Buchli, J. and Schaal, S.},
booktitle={Robotics and Automation (ICRA), 2010 IEEE International Conference on},
title={Reinforcement learning of motor skills in high dimensions: A path integral approach},
year={2010},
month={May},
pages={2397-2403},
keywords={control engineering computing;intelligent robots;learning (artificial intelligence);learning systems;optimal control;stochastic systems;complex motor system;continuous state-action spaces;estimation theory;gradient-based policy learning;high-dimensional control problem;learning control;motor skills;parameterized control policy;path integral approach;path integrals;policy improvement;reinforcement learning;robot dog;stochastic optimal control;Control systems;Function approximation;Inference algorithms;Integral equations;Learning systems;Optimal control;Robots;Scalability;Stochastic processes;Stochastic resonance},
doi={10.1109/ROBOT.2010.5509336},
ISSN={1050-4729},}

@techreport{ijspeert2002learning,
  title={Learning attractor landscapes for learning motor primitives},
  author={Ijspeert, Auke Jan and Nakanishi, Jun and Schaal, Stefan},
  year={2002}
}

@article{AlTamimi2007473,
title = "Model-free Q-learning designs for linear discrete-time zero-sum games with application to H-infinity control ",
journal = "Automatica ",
volume = "43",
number = "3",
pages = "473 - 481",
year = "2007",
note = "",
issn = "0005-1098",
doi = "http://dx.doi.org/10.1016/j.automatica.2006.09.019",
url = "http://www.sciencedirect.com/science/article/pii/S0005109806004249",
author = "Asma Al-Tamimi and Frank L. Lewis and Murad Abu-Khalaf",
keywords = "Adaptive critics",
keywords = "Approximate dynamic programming",
keywords = "Zero-sum games",
keywords = "Policy iterations",
keywords = "H ∞ optimal control",
keywords = "Q -function",
keywords = "Q -learning",
keywords = "Adaptive control "
}

@misc{UR5,
  title = {UR5 Technical Specification},
  howpublished = {\url{http://files.mediastorage.dk/UR_Tech_Spec/UR5_EN.pdf}},
  note = {Accessed: 2015-02-11}
}

@misc{scanCONTROL,
  title = {scanCONTROL Technical Detail},
  howpublished = {\url{http://sentec.jp/products/pdf/LLT2700_2750_2710.pdf}},
  note = {Accessed: 2015-02-11}
}

@book{sunniva2013,
  title={On-Surface 3D Printing Method using a Robotic Arm},
  author={S. van Ipenburg},
  year={2013},
  publisher={Thesis Report, MSc BioMechanical Engineering, TU Delft}
}

@book{Gier2013,
  title={UR5 MPC Program},
  author={M. de Gier},
  year={2014},
  publisher={DCSC, TU Delft}
}

@book{verhaegen2007filtering,
  title={Filtering and system identification: a least squares approach},
  author={Verhaegen, Michel and Verdult, Vincent},
  year={2007},
  publisher={Cambridge university press}
}

@INPROCEEDINGS{6942896,
author={Qujiang Lei and Wisse, M.},
booktitle={Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on},
title={Fast grasping of unknown objects using force balance optimization},
year={2014},
month={Sept},
pages={2454-2460},
keywords={grippers;optimisation;Universal arm UR5;force balance optimization;point cloud information;robot hand;underactuated Lacquey Fetch gripper;unknown object grasping;Conferences;Intelligent robots},
doi={10.1109/IROS.2014.6942896},}

@INPROCEEDINGS{856947,
author={Howell, M.N. and Gordon, T.J. and Best, M.C.},
booktitle={Learning Systems for Control (Ref. No. 2000/069), IEE Seminar},
title={The application of continuous action reinforcement learning automata to adaptive PID tuning},
year={2000},
month={},
pages={2/1-2/4},
keywords={three-term control;CARLA;Ford Zetec 1.8 engine;Gaussian distribution;PID controller;PID controller parameter tuning;Zeigler-Nichols methods;adaptive PID tuning;continuous action reinforcement learning automata;convergent distribution;engine idle speed control system;engine stall;idle speed control;load change disturbances;parameter tuning;performance criterion minimisation;stochastic parameter selection;system nonlinearities;time error square;vehicle emission reduction},
doi={10.1049/ic:20000343},}


@INPROCEEDINGS{Sedighizadeh2008,
author={M. Sedighizadeh, and A. Rezazadeh},
booktitle={World Academy of Science, Engineering and Technology Vol:2},
title={Adaptive PID Controller based on Reinforcement Learning for Wind Turbine Control},
year={2008},
month={},}


